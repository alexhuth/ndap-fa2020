{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro Data Analysis in Python: Problem Set 6\n",
    "\n",
    "This is the sixth problem set. It has 3 problems, worth a total of 44 points. It is due before class (i.e. by 10:59 AM) on 12/4/2020. For late policy please see [the syllabus](https://github.com/alexhuth/ndap-fa2020/blob/master/README.md#late-homework--extension-policy). Partial credit will be awarded for partially correct solutions.\n",
    "\n",
    "\n",
    "## Homework submission\n",
    "\n",
    "When you've finished, rename the notebook file to `ndap-problem_set_6-YOUREID.ipynb`. For example, if your EID is `ab12345`, you should call it `ndap-problem_set_6-ab12345.ipynb`. Then upload your completed problem set to canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. (8 pts)\n",
    "\n",
    "Solve these small regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this first to set up variables\n",
    "b_true = np.array([1, 0.25, 0.5, 1.25, 0])\n",
    "X = np.random.randn(100, 5)\n",
    "y = X.dot(b_true) + 0.25*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ordinary least squares regression (np.linalg.lstsq)\n",
    "# to fit a regression model that predicts y from X\n",
    "# store the weights, residuals, rank, and singular values in\n",
    "# separate variables (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute (in-set) R^2 for the fit model using X and y (2 pts)\n",
    "# (it should be close to 1.0)\n",
    "\n",
    "#R_2 = # ?\n",
    "print(R_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot showing the true weights (b_true)\n",
    "# on the x-axis and the estimated weights on the y-axis\n",
    "# use the format string 'o'\n",
    "# label the x- and y-axes (2 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot showing the true y-values (y)\n",
    "# on the x-axis and the predicted y-values on the y-axis\n",
    "# again, use the format string 'o'\n",
    "# label the x- and y-axes (2 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. (6 pts)\n",
    "\n",
    "Here you'll be using regression to fit models to fMRI data! In the next few problem, you will learn about how to use linear regression to fit _filters_ that you can use to model timeseries. You'll be using linear regression techniques to model fMRI responses to video stimuli (the ones that we've talked about in class) based on the presence of two categories in the videos: people and buildings.\n",
    "\n",
    "This problem will just involve loading and doing basic visualizations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fMRI dataset\n",
    "datafile = np.load('ps6_data.npz')\n",
    "\n",
    "# list all the variables in the file\n",
    "print(datafile.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "# (you don't need to worry about subtracting the mean \n",
    "# later because that's done here)\n",
    "\n",
    "# the features (X) say whether people (column zero) or buildings \n",
    "# (column one) are present in each video clip\n",
    "X_trn = datafile['X_trn'] # time x features\n",
    "X_trn -= X_trn.mean(0) # subtract the mean over time\n",
    "X_test = datafile['X_test'] # time x features\n",
    "X_test -= X_test.mean(0) # ditto\n",
    "\n",
    "# y_trn and y_test have the fMRI response of one voxel\n",
    "y_trn = datafile['y_trn'] # time\n",
    "y_trn -= y_trn.mean(0)\n",
    "y_test = datafile['y_test'] # time\n",
    "y_test -= y_test.mean(0)\n",
    "\n",
    "# ybig_trn and ybig_test have the response of 10,000 voxels\n",
    "ybig_trn = datafile['ybig_trn']\n",
    "ybig_trn -= ybig_trn.mean(0)\n",
    "ybig_test = datafile['ybig_test']\n",
    "ybig_test -= ybig_test.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Print the shape of each array (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shapes of X_trn, X_test, y_trn, y_test, ybig_trn, ybig_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Simple plots (2 pts)\n",
    "Step 1 in any analysis should _always_ be to **LOOK AT YOUR DATA**. Let's plot it and see what it looks like: what's the range, does it look uniform, etc. Plot the regression target (`y_trn`) and both regression features (`X_trn[:,0]` and `X_trn[:,1]`) on the same axis. Label the x-axis. Assign labels to each line (using the `label=` keyword in `plt.plot`) and then add a legend using `plt.legend()` so we know which line is which.\n",
    "\n",
    "Then plot a histogram of the values in `y_trn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot responses & features\n",
    "\n",
    "# histogram y_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Simple correlation (2 pts)\n",
    "Compute and print the correlation between `y_trn` and `X_trn[:,0]`, and the correlation between `y_trn` and `X_trn[:,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute & print correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3. (30 pts)\n",
    "\n",
    "One of the key things you should know about fMRI is that it measures the Blood-Oxygen-Level Dependent (BOLD) signal, which tells you about blood flow in each area of the brain. This signal doesn't directly track neural activity! After there is a burst of neural activity in an area, nearby blood vessels slowly respond and recruit more blood over the next 2-8 seconds. So we can't directly model the fMRI response with our stimuli! We need to account for the slow [hemodynamic response](https://en.wikipedia.org/wiki/Haemodynamic_response).\n",
    "\n",
    "We can think of the hemodynamic response as (to first approximation) a _filter_ on the underlying neural activity. So if the stimulus features (here, the presence of people or buildings in a video) are correlated with the neural activity, we can try to find a filter that we can apply to the stimulus features in order to make them look like the BOLD response.\n",
    "\n",
    "We'll do this by creating new features that are _lagged_ versions of the stimulus features. If we use lags (0,1,2,3,4,5) then it's like we're modeling the response $y_t$ as a function of the stimulus features $x_t, x_{t-1}, x_{t-2}, x_{t-3}, x_{t-4}, x_{t-5}$. This is just like fitting a filter that is (effectively) convolved with the stimulus feature to get the response!\n",
    "\n",
    "This approach is called a **Finite Impulse Response (FIR) model**.\n",
    "\n",
    "## (a) Create FIR regressors (9 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1 pt)\n",
    "# let's start with just one feature, feature zero\n",
    "# create a matrix that will hold 6 lagged versions of the feature vector\n",
    "# this matrix should have T rows (same as X_trn) and 6 columns\n",
    "X0_trn_lagged = np.zeros((###SOMETHING###))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4 pts)\n",
    "# now copy the feature X_trn[:,0] into each of the 6 columns of your new matrix\n",
    "# we want the first column to be exactly X_trn[:,0] (because it's lag=0),\n",
    "# the second column to be lagged by 1 timepoint relative to the original,\n",
    "# the third to be lagged lagged 2, etc.\n",
    "\n",
    "# The for loop is already set up so that:\n",
    "# `ii` is the index of the column in `X0_trn_lagged` you want to copy into\n",
    "# `lag` is how much this column should be lagged\n",
    "\n",
    "# you just need to find and fill in the `frm` and `to` variables!\n",
    "\n",
    "for ii,lag in enumerate([0,1,2,3,4,5]):\n",
    "    # `frm` should be the timepoint that you start copying from in X_trn\n",
    "    frm = ## SOMETHING ##\n",
    "    # and `to` is the last timepoint\n",
    "    to = ## SOMETHING\n",
    "    X0_trn_lagged[lag:,ii] = X_trn[frm:to,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# plot the first 100 timepoints for each of these 6 feature vectors\n",
    "# on the same axis\n",
    "# they should look identical, but shifted in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# now use the same logic to create X0_test_lagged from X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Fit the FIR model (6 pts)\n",
    "Next, use `np.linalg.lstsq` to fit a linear regression model that predicts `y_trn` from `X0_trn_lagged`. Save the `wt, rank, res, sing` as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# do regression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt, rank, res, sing = np.linalg.lstsq(X0_trn_lagged, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# now plot the weights as dots connected by lines\n",
    "# make the x-axis values equal to the lags (0..5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# write, in 1 sentence:\n",
    "# * What do these weights mean?\n",
    "# * Why do they look like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Test the FIR model (3 pts)\n",
    "Use the dot product between `wt` and `X0_test_lagged` to predict responses in the test dataset, then compute the correlation between predicted and actual responses (`y_test`). How well does our model work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# create predicted test timecourse\n",
    "y_test_pred = ## SOMETHING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1 pt)\n",
    "# compute correlation\n",
    "model1_corr = ## SOMETHING ##\n",
    "print(model1_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Repeat procedure in big dataset (12 pts)\n",
    "The model above was fit to just one voxel. Let's repeat this process for each of the 10,000 voxels in the bigger dataset. Ideally we would do this by reshaping the `ybig_*` datasets into a more sensible shape, but it might be easier to do using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4 pts)\n",
    "wt_big = np.zeros((6, 100, 100)) # this matrix will hold the weights\n",
    "for ii in range(100):\n",
    "    for jj in range(100):\n",
    "        ## DO THE REGRESSION FOR VOXEL [ii,jj] ##\n",
    "        wt_big[:,ii,jj] = ## THE WEIGHTS FOR THAT REGRESSION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# compute predictions & find the prediction correlation \n",
    "# for every voxel in the big set\n",
    "corr_big = np.zeros((100,100))\n",
    "for ii in range(100):\n",
    "    for jj in range(100):\n",
    "        corr_big[ii,jj] = ## SOMETHING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 pts)\n",
    "# plot the matrix `corr_big` using plt.matshow\n",
    "# set vmin=0, and vmax=0.6\n",
    "# this shows correlation across one axial slice through a subject's brain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4 pts)\n",
    "# let's make the plot look nicer\n",
    "# first get something resembling brain anatomy by using ybig_test.std(0) \n",
    "# (this just shows where \"high signal\" voxels are, which are all in cortex)\n",
    "# plot that using plt.matshow with a grayscale colormap\n",
    "\n",
    "\n",
    "# next, create a \"thresholded\" version of your correlation map\n",
    "# to do this, first create a copy (remember .copy()?)\n",
    "# then set all the values in the copy that are below 0.3 to np.nan\n",
    "# finally, use plt.imshow to plot the thresholded correlations on top of the \"anatomy\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
